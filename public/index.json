[{"authors":null,"categories":null,"content":"Alex works as a Data Scientist in a Business Intelligence company and usually collaborates in ESPOL research projects. He also has experience working with Social Media and Financial data. And he is interested in developing research involving Econometrics and Machine Learning (intersection of fields).\n  Download my resum√©.\nPrimary interests:  Applied Econometrics Networks Machine Learning Data Science  Secondary interests:  Causal Inference Applied Microeconomics Social Media  ","date":1647302400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1647302400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://alexamaguaya.netlify.app/author/alex-amaguaya/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alex-amaguaya/","section":"authors","summary":"Alex works as a Data Scientist in a Business Intelligence company and usually collaborates in ESPOL research projects. He also has experience working with Social Media and Financial data. And he is interested in developing research involving Econometrics and Machine Learning (intersection of fields).","tags":null,"title":"Alex Amaguaya","type":"authors"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor Alex Amaguaya FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://alexamaguaya.netlify.app/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"https://alexamaguaya.netlify.app/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"https://alexamaguaya.netlify.app/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"https://alexamaguaya.netlify.app/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://alexamaguaya.netlify.app/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Jos√© Gabriel Castillo","Juan Carlos Campuzano","Alex Amaguaya","N√©mesis G√≥mez"],"categories":null,"content":"","date":1647302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647302400,"objectID":"2ee8a49c2544adb9a7242a8bf22bb1f8","permalink":"https://alexamaguaya.netlify.app/publication/report_espol_idb/","publishdate":"2021-12-01T00:00:00Z","relpermalink":"/publication/report_espol_idb/","section":"publication","summary":"This report on the economic and social contribution of ESPOL in the Litoral region is a pioneering initiative, both locally and regionally, of the evaluation and measurement of the impacts generated by university activity within the society in which it is developed, in the short and long term. It is worth mentioning that the preparation of this study takes place in the midst of one of the most serious economic crises the country has faced, which has had strong implications on the higher education system. In the face of this adverse scenario, the estimation of the effects derived from university activity constitutes today more than ever the necessary starting point to establish the university as a strategic ally of countries seeking to achieve better development conditions. Thus, the main objective of this study is to evaluate the contribution of the university to economic and social restoration, through the expenditure generated in the interaction with the different economic sectors involved in the development of its training, research, innovation and outreach activities. Additionally, long-term contributions are considered, such as the contribution to the creation of human capital, creation of technological capital, contribution to the business fabric, among others.","tags":["Applied Econometrics","Applied Economics"],"title":"Evaluaci√≥n de la contribuci√≥n econ√≥mica de la Escuela Superior Polit√©cnica del Litoral - ESPOL: Un an√°lisis de corto y largo plazo","type":"publication"},{"authors":["Alex Amaguaya","Carmen Vaca","Fanny L√∫a"],"categories":null,"content":"","date":1647302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647302400,"objectID":"b4abfdc1d3affe3d21b502e668b3f438","permalink":"https://alexamaguaya.netlify.app/publication/manager_network/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/publication/manager_network/","section":"publication","summary":"Information has always been a potential source of comparative advantage between companies, so knowing it in its entirety  has  become  a  necessity  for  decision  makers.  Today,  there  are  tools  such  as  Social  Network  Analysis, which  may  have  a  greater  scope  to  know  the  impact  of  relationships  between  individuals.  In  this  project,  the effect  of  financial  performance  on  sharing  human  capital  and  being  a  prominent  company  in  the  market  is quantified.  Likewise,  communities  in  the  industry  were  identified,  using  modularity  as  a  measure  of  grouping between companies. With a sample of 9,070 signatures from Ecuador in 2018, indicators were proposed based on  the  history  of  administrators  and  the  centrality  measures  of  the  graph  theory.  Income  was  chosen  as  the indicator  that  best  represents  financial  performance.  The  results  found  showed  that  there  is  a  considerable positive  effect  on  financial  performance  when  a  company  is  prominent  in  the  market.  On  the  other  hand,  it  is determined  that  there  is  a  negative  effect  on  financial  performance  by  sharing  human  capital,  when  hiring  a worker  who  works  or  worked  in  a  company  in  the  same  sector,  as  well  as  in  other  sectors  of  the  industry. Finally, it was found that the indicators that quantify connections with companies represent avaluable source of information for firms.","tags":["Networks","Applied Econometrics","Microeconomics"],"title":"Red de Administradores Compartidos y su Relaci√≥n con el Desempe√±o Financiero en Empresas del Ecuador: ¬øCu√°l es el efecto de compartir Capital Humano?","type":"publication"},{"authors":["Alex Amaguaya","Johnny Cajape","Juan Carlos Campuzano"],"categories":null,"content":"","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646092800,"objectID":"7758a1dc7facc92ae8f6a545e34d6080","permalink":"https://alexamaguaya.netlify.app/publication/social_media/","publishdate":"2020-03-01T00:00:00Z","relpermalink":"/publication/social_media/","section":"publication","summary":"After the arrival of social networks and the interconnection that it caused among its users, the generation of businesses  within  these  platforms  was  inevitable,  the main  one  of  them  selling  advertising,  in  Ecuador  the sale  of  advertising  through  social  networks  amounts  to  a  amount  exceeding  thirty  million  dollars  only  for companies  in  the  commerce  sector,  therefore,  this  paper  analyzes  the  efficiency  of  advertising  spending through the condition of Dorfman-Steiner for eighty-eight companies in the commerce sector during 2018, generating  indicators  of  Management  for  digital  marketing  (KPI's)  using  Machine  Learning  techniques  for the  processing  of  data  from  the  social  network  twitter  and  relating  them  to  the  financial  results  of  these companies  during  the  same  period,  through  a  multiple  linear  regression.  In  the  analysis  performed,  a significant effect was found by the indicators towards the Dorfman-Steiner condition for companies with a small  number  of  tweets,  the  greatest  effects  found  were  given  through  the  interaction  between  the indicators concluding that, to reduce the level Advertising spending should be aimed at the propagation and popularity of the content that is published, taking into account the quality of the content that is disclosed.Key Words:Dorfman-Steiner, KPI‚Äôs, machine learning, twitter.","tags":["Applied Econometrics","Social Media"],"title":"Eficiencia del Gasto en Publicidad Analizado a trav√©s de Datos en Twitter Para Empresas del Sector Comercio en el Periodo 2018","type":"publication"},{"authors":null,"categories":"R","content":"  body { font-size: 14pt; } h1 { /* Header 2 */ font-size: 32px; color: #168d1e; font-weight: bold; } h2 { /* Header 1 */ font-size: 24px; color: #dc7b1e; font-weight: bold; }  Stream Listener of Twitter This is a fast and easy example about how download twitter data.\nImports import tweepy from tweepy import Stream import pandas as pd import json from datetime import datetime   Functions def get_tweet_type(tweet): source_tweet = False type_tw = \u0026quot;TW\u0026quot; try: tmp = tweet[\u0026#39;retweeted_status\u0026#39;] source_tweet = tmp[\u0026#39;id\u0026#39;] type_tw = \u0026quot;RT\u0026quot; except: pass try: tmp = tweet[\u0026#39;quoted_status\u0026#39;] source_tweet = tmp[\u0026#39;id\u0026#39;] type_tw = \u0026quot;QT\u0026quot; except: pass try: tmp = tweet[\u0026#39;in_reply_to_status_id\u0026#39;] if tmp != None and type == \u0026quot;TW\u0026quot;: type_tw = \u0026quot;RP\u0026quot; source_tweet = tmp except: pass return type_tw, source_tweet  Class  Twitter Stream Listener  class StdOutListener(Stream): \u0026quot;\u0026quot;\u0026quot; This is a listener that just prints received tweets to stdout. \u0026quot;\u0026quot;\u0026quot; def __init__(self, fetched_tweets_filename): self.fetched_tweets_filename = fetched_tweets_filename def on_data(self, data): try: print(data) data = json.loads(data) if None == data[\u0026quot;in_reply_to_screen_name\u0026quot;]: in_reply_to_screen_name = \u0026#39;nan\u0026#39; else: in_reply_to_screen_name = str(data[\u0026quot;in_reply_to_screen_name\u0026quot;]) try: ls_dict = data[\u0026#39;entities\u0026#39;][\u0026#39;user_mentions\u0026#39;] user_mentions = \u0026#39;~\u0026#39;.join( [ v_dict[\u0026#39;screen_name\u0026#39;] for v_dict in ls_dict ] ) except: user_mentions = \u0026#39;nan\u0026#39; try: ls_dict = data[\u0026#39;entities\u0026#39;][\u0026#39;hashtags\u0026#39;] hash_mentions = \u0026#39;~\u0026#39;.join( [ v_dict[\u0026#39;text\u0026#39;] for v_dict in ls_dict ] ) except: hash_mentions = \u0026#39;nan\u0026#39; text = (data[\u0026quot;text\u0026quot;].strip()).replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) created = data[\u0026quot;created_at\u0026quot;] created = datetime.strftime(datetime.strptime(created, \u0026#39;%a %b %d %H:%M:%S +0000 %Y\u0026#39;), \u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) type_tw, source_tweet = get_tweet_type(data) retweets = data[\u0026#39;retweet_count\u0026#39;] favorites = data[\u0026#39;favorite_count\u0026#39;] quote_counts = data[\u0026quot;quote_count\u0026quot;] lang = data[\u0026#39;lang\u0026#39;] u_id = data[\u0026quot;user\u0026quot;][\u0026quot;id_str\u0026quot;] u_screen_name = data[\u0026quot;user\u0026quot;][\u0026quot;screen_name\u0026quot;] u_followers = data[\u0026quot;user\u0026quot;][\u0026quot;followers_count\u0026quot;] u_followings = data[\u0026quot;user\u0026quot;][\u0026quot;friends_count\u0026quot;] u_location = data[\u0026#39;user\u0026#39;][\u0026#39;location\u0026#39;] if (data[\u0026#39;user\u0026#39;][\u0026#39;description\u0026#39;]) != None: u_bio = (data[\u0026#39;user\u0026#39;][\u0026#39;description\u0026#39;]).replace(\u0026#39;\\n\u0026#39;,\u0026#39;\u0026#39;) else: u_bio = \u0026#39;nan\u0026#39; u_created = data[\u0026#39;user\u0026#39;][\u0026#39;created_at\u0026#39;] u_created = datetime.strftime(datetime.strptime(u_created,\u0026#39;%a %b %d %H:%M:%S +0000 %Y\u0026#39;), \u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;) new_ls = [ created , text, type_tw, retweets, favorites, quote_counts,lang, in_reply_to_screen_name, user_mentions, hash_mentions, u_id, u_screen_name, u_followers, u_followings, u_location, u_bio, u_created] new_ls = [ \u0026#39;nan\u0026#39; if v == None else v for v in new_ls ] str_data = \u0026#39;~|~\u0026#39;.join(map(str, new_ls)) with open(self.fetched_tweets_filename, \u0026#39;a\u0026#39;) as tf: tf.write(str_data+\u0026#39;\\n\u0026#39;) return True except BaseException as e: print(\u0026quot;Error on_data %s\u0026quot; % str(e)) return True def on_error(self, status): print(status)  Twitter Streamer  class TwitterStreamer(): \u0026quot;\u0026quot;\u0026quot; Class for streaming and processing live tweets and filter by a location and hashtags. \u0026quot;\u0026quot;\u0026quot; def __init__(self): pass def stream_tweets(self, fetched_tweets_filename, hash_tag_list): # This handles Twitter authetification and the connection to Twitter Streaming API listener = StdOutListener(fetched_tweets_filename) auth = tweepy.OAuthHandler(\u0026quot;*******\u0026quot;, \u0026quot;*******\u0026quot;) auth.set_access_token( \u0026quot;*******\u0026quot;, \u0026quot;*******\u0026quot;) stream = Stream(auth, listener) # This line attempt to filter Twitter Streams and capture data by the keywords/hashtags: stream.filter(track=hash_tag_list,locations = [00, 00, 00, 00] ) The following section is executed from the terminal and can change some parameters such as:\n File name Column names Hashtags Location Twitter Passwords  if __name__ == \u0026#39;__main__\u0026#39;: hash_tag_list = [\u0026quot;********\u0026quot;, \u0026quot;********\u0026quot;, \u0026quot;********\u0026quot;] fetched_tweets_filename = \u0026quot;********.txt\u0026quot; twitter_streamer = TwitterStreamer() ls_columns_name = [\u0026quot;created_at\u0026quot;,\u0026quot;text\u0026quot;,\u0026quot;type_tw\u0026quot;,\u0026quot;retweets\u0026quot;,\u0026quot;favorites\u0026quot;,\u0026quot;quotes_count\u0026quot;,\u0026#39;lang\u0026#39;, \u0026quot;in_reply_to_screen_name\u0026quot;, \u0026#39;users_mentions\u0026#39;, \u0026#39;hashtags_mentions\u0026#39;, \u0026quot;user_id\u0026quot;, \u0026quot;user_screen_name\u0026quot;,\u0026quot;user_followers\u0026quot;,\u0026quot;user_followings\u0026quot;,\u0026quot;user_location\u0026quot;, \u0026quot;user_bio\u0026quot;, \u0026quot;user_created_at\u0026quot;] str_colnames = \u0026#39;~|~\u0026#39;.join(ls_columns_name) with open(fetched_tweets_filename, \u0026#39;w\u0026#39;) as tf: tf.write(str_colnames) tf.write(\u0026#39;\\n\u0026#39;) twitter_streamer.stream_tweets(fetched_tweets_filename, hash_tag_list)  ","date":1644520514,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644520514,"objectID":"eb60b46f8d3fbf0e7081816a1464310e","permalink":"https://alexamaguaya.netlify.app/blog/twitter_listener/","publishdate":"2022-02-10T14:15:14-05:00","relpermalink":"/blog/twitter_listener/","section":"blog","summary":"This blog is an example about how to download Twitter data of different topics.","tags":["Social Media","Python"],"title":"Twitter Streaming","type":"blog"},{"authors":null,"categories":"R","content":"  body { font-size: 14pt; } h1 { /* Header 2 */ font-size: 32px; color: DarkBlue; font-weight: bold; } h2 { /* Header 1 */ font-size: 24px; color: #5eae31; font-weight: bold; }  Time Series Example The purpose of this bog is compare the performance of some forecast methods in order to select the best option.\nImport packages and additional functions library(dplyr) library(readr) library(forecast) library(xts) # functions rmse \u0026lt;- function(real,prediction){ error \u0026lt;- real - prediction return( sqrt(mean(error^2)) ) } plot_df \u0026lt;- function(df, title_text){ plot(xts( df[,c(\u0026#39;real\u0026#39;,\u0026#39;prediction\u0026#39;)], order.by = df$row ) , type = \u0026quot;l\u0026quot;, main = title_text, col = c(\u0026quot;#ff3644\u0026quot;,\u0026quot;#2f002f\u0026quot;), ylab = \u0026#39;values\u0026#39;, legend.loc = \u0026quot;bottomleft\u0026quot;, lty = c(\u0026quot;solid\u0026quot;,\u0026quot;dashed\u0026quot;) ) } create_df_compare \u0026lt;- function(real, prediction){ df \u0026lt;- data.frame(real= real ,prediction = prediction, row = seq(as.Date(\u0026#39;1987-03-01\u0026#39;), length = length(real), by = \u0026quot;month\u0026quot;) ) return(df) }  Load data set First, I loaded the .dat file and then plotted the time series to make a visual analysis.\nvariable \u0026lt;- round(read.delim(\u0026#39;serie.dat\u0026#39;,header=FALSE),2) cat(\u0026#39;The file has: \u0026#39;, nrow(variable), \u0026#39;observations.\\n\u0026#39;) ## The file has: 168 observations. summary(variable) ## V1 ## Min. :20.00 ## 1st Qu.:23.28 ## Median :24.95 ## Mean :25.06 ## 3rd Qu.:26.88 ## Max. :30.00 plot(xts(x= variable$V1, order.by = seq(as.Date(\u0026#39;1976-01-01\u0026#39;), length = length(variable$V1), by = \u0026quot;month\u0026quot;)), type=\u0026quot;l\u0026quot;, col=\u0026quot;#2f002f\u0026quot;, lwd=2, main = \u0026#39;Evolution of Time serie\u0026#39;, ylab = \u0026#39;Values\u0026#39;)  Split in train and test data set Subsequently, I divided the total data in train (80%) and test (20%).\n The Train data had 134 observations (from ‚Äú1976-01-01‚Äù to ‚Äú1987-02-01‚Äù). The Train data had 34 observations (from ‚Äú1987-02-01‚Äù to ‚Äú1989-11-01‚Äù).  Also, I show the plots about train and test data.\ntrain_df \u0026lt;- variable[1:round(length(variable$V1)*0.8,0), ] test_df \u0026lt;- variable[(round(length(variable$V1)*0.8,0)+1): nrow(variable), ] par(mfrow=c(2,1), mar=c(3,4,3,1)) plot(xts(x= train_df, order.by = seq(as.Date(\u0026#39;1976-01-01\u0026#39;), length = length(train_df), by = \u0026quot;month\u0026quot;) ) , type=\u0026quot;l\u0026quot;, col=\u0026quot;#2f002f\u0026quot;, main =\u0026#39;time series variable- train\u0026#39;) plot(xts(x= test_df, order.by = seq(as.Date(\u0026#39;1987-02-01\u0026#39;), length = length(test_df), by = \u0026quot;month\u0026quot;)), type=\u0026quot;l\u0026quot;, col=\u0026quot;#2f002f\u0026quot;, main =\u0026#39;time series variable-test\u0026#39;)   Time serie Descomposition descomp \u0026lt;- decompose(ts(train_df,frequency = 12,start = 1976)) plot(descomp)  Models Arima I used an auto arima with a Box-Cox transformation. The model had for the the Non-seasonal part the following results: p = 0, d=1, q= 0, while for the seasonal part: p = 2, d=1, q= 2. In summary, first difference had to be applied to the variable and a relevant seasonal pattern was detected.\narima_model \u0026lt;- auto.arima(ts(train_df,frequency = 12,start = 1976), lambda=\u0026#39;auto\u0026#39;) print(arima_model) ## Series: ts(train_df, frequency = 12, start = 1976) ## ARIMA(0,1,0)(2,1,1)[12] ## Box Cox transformation: lambda= 0.6710961 ## ## Coefficients: ## sar1 sar2 sma1 ## -0.4372 -0.2523 -0.7566 ## s.e. 0.1371 0.1330 0.1711 ## ## sigma^2 estimated as 0.04618: log likelihood=4.59 ## AIC=-1.18 AICc=-0.83 BIC=10  Drift model The drift model was estimated with a Box-Cox transformation. Below I show the specification on this model.\n## ## Forecast method: Random walk with drift ## ## Model Information: ## Call: rwf(y = ts(train_df, frequency = 12, start = 1976), h = length(test_df), ## ## Call: drift = TRUE, level = c(80, 95), fan = FALSE, lambda = \u0026quot;auto\u0026quot;, ## ## Call: biasadj = TRUE, bootstrap = FALSE, npaths = 5000, x = train_df) ## ## Drift: -0.0351 (se 0.3279) ## Residual sd: 3.7821 ## ## Error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set NaN NaN NaN NaN NaN NaN NA ## ## Forecasts: ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 135 24.82334 22.91256 26.71743 21.8750943 27.69769 ## 136 24.79646 22.07730 27.48177 20.5835450 28.85925 ## 137 24.76935 21.41963 28.06759 19.5616767 29.74826 ## 138 24.74203 20.85232 28.56210 18.6761833 30.49792 ## 139 24.71449 20.34176 28.99891 17.8756320 31.15954 ## 140 24.68673 19.87078 29.39515 17.1337141 31.75932 ## 141 24.65874 19.42921 29.76099 16.4348183 32.31272 ## 142 24.63053 19.01047 30.10300 15.7687649 32.82978 ## 143 24.60210 18.60997 30.42572 15.1284596 33.31744 ## 144 24.57344 18.22441 30.73245 14.5087040 33.78072 ## 145 24.54456 17.85129 31.02567 13.9055328 34.22340 ## 146 24.51546 17.48867 31.30730 13.3158176 34.64840 ## 147 24.48613 17.13502 31.57884 12.7370165 35.05801 ## 148 24.45657 16.78908 31.84152 12.1670079 35.45411 ## 149 24.42679 16.44984 32.09635 11.6039749 35.83823 ## 150 24.39678 16.11643 32.34416 11.0463219 36.21163 ## 151 24.36654 15.78812 32.58565 10.4926125 36.57538 ## 152 24.33607 15.46428 32.82142 9.9415191 36.93040 ## 153 24.30537 15.14438 33.05198 9.3917810 37.27746 ## 154 24.27445 14.82793 33.27777 8.8421675 37.61724 ## 155 24.24329 14.51452 33.49919 8.2914403 37.95033 ## 156 24.21191 14.20377 33.71658 7.7383153 38.27726 ## 157 24.18029 13.89535 33.93022 7.1814166 38.59847 ## 158 24.14844 13.58894 34.14040 6.6192202 38.91437 ## 159 24.11635 13.28427 34.34735 6.0499767 39.22532 ## 160 24.08404 12.98108 34.55128 5.4716017 39.53166 ## 161 24.05149 12.67914 34.75239 4.8815051 39.83367 ## 162 24.01870 12.37821 34.95084 4.2763124 40.13161 ## 163 23.98568 12.07810 35.14680 3.6513664 40.42573 ## 164 23.95243 11.77860 35.34041 2.9997411 40.71625 ## 165 23.91893 11.47951 35.53180 2.3099795 41.00336 ## 166 23.88520 11.18067 35.72110 1.5595211 41.28725 ## 167 23.85124 10.88188 35.90840 0.6833618 41.56809 ## 168 23.81703 10.58297 36.09381 -0.6229830 41.84602  Naive model The naive model was estimated with a Box-Cox transformation. Below I show the specification on this model.\nnaive_model \u0026lt;- naive(ts(train_df,frequency = 12,start = 1976), level = c(80, 95), fan = FALSE, lambda = \u0026quot;auto\u0026quot;, biasadj = TRUE, bootstrap = FALSE, npaths = 5000, x = train_df, h = length(test_df) ) summary(naive_model) ## ## Forecast method: Naive method ## ## Model Information: ## Call: naive(y = ts(train_df, frequency = 12, start = 1976), h = length(test_df), ## ## Call: level = c(80, 95), fan = FALSE, lambda = \u0026quot;auto\u0026quot;, biasadj = TRUE, ## ## Call: bootstrap = FALSE, npaths = 5000, x = train_df) ## ## Residual sd: 3.768 ## ## Error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set NaN NaN NaN NaN NaN NaN NA ## ## Forecasts: ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 135 24.83720 22.93389 26.72396 21.900597 27.70052 ## 136 24.82439 22.12656 27.48896 20.645047 28.85615 ## 137 24.81159 21.50122 28.07177 19.666381 29.73381 ## 138 24.79879 20.96989 28.56037 18.830186 30.46789 ## 139 24.78598 20.49852 28.98884 18.084426 31.11038 ## 140 24.77318 20.06963 29.37461 17.402417 31.68792 ## 141 24.76038 19.67285 29.72807 16.768303 32.21633 ## 142 24.74757 19.30144 30.05597 16.171745 32.70591 ## 143 24.73477 18.95069 30.36299 15.605550 33.16379 ## 144 24.72197 18.61720 30.65255 15.064469 33.59517 ## 145 24.70916 18.29839 30.92722 14.544525 34.00399 ## 146 24.69636 17.99224 31.18900 14.042617 34.39327 ## 147 24.68356 17.69718 31.43948 13.556263 34.76544 ## 148 24.67075 17.41190 31.67995 13.083435 35.12246 ## 149 24.65795 17.13535 31.91148 12.622447 35.46594 ## 150 24.64515 16.86663 32.13494 12.171871 35.79723 ## 151 24.63234 16.60498 32.35109 11.730485 36.11747 ## 152 24.61954 16.34977 32.56057 11.297220 36.42763 ## 153 24.60674 16.10044 32.76393 10.871139 36.72856 ## 154 24.59393 15.85651 32.96165 10.451401 37.02097 ## 155 24.58113 15.61754 33.15416 10.037246 37.30552 ## 156 24.56833 15.38317 33.34182 9.627980 37.58275 ## 157 24.55552 15.15305 33.52497 9.222955 37.85318 ## 158 24.54272 14.92689 33.70389 8.821563 38.11725 ## 159 24.52992 14.70443 33.87886 8.423225 38.37536 ## 160 24.51711 14.48542 34.05010 8.027378 38.62787 ## 161 24.50431 14.26964 34.21784 7.633467 38.87509 ## 162 24.49151 14.05689 34.38227 7.240938 39.11734 ## 163 24.47870 13.84700 34.54356 6.849226 39.35487 ## 164 24.46590 13.63978 34.70189 6.457743 39.58794 ## 165 24.45310 13.43509 34.85739 6.065865 39.81676 ## 166 24.44029 13.23279 35.01020 5.672920 40.04155 ## 167 24.42749 13.03274 35.16046 5.278158 40.26249 ## 168 24.41469 12.83482 35.30827 4.880733 40.47976  Snaive model The snaive model was estimated with a Box-Cox transformation. Below I show the specification on this model.\nsnaive_model \u0026lt;- snaive(ts(train_df,frequency = 12,start = 1976), level = c(80, 95), fan = FALSE, lambda = \u0026#39;auto\u0026#39;, biasadj = TRUE, bootstrap = FALSE, npaths = 5000, x = train_df, h = length(test_df) ) summary(snaive_model) ## ## Forecast method: Seasonal naive method ## ## Model Information: ## Call: snaive(y = ts(train_df, frequency = 12, start = 1976), h = length(test_df), ## ## Call: level = c(80, 95), fan = FALSE, lambda = \u0026quot;auto\u0026quot;, biasadj = TRUE, ## ## Call: bootstrap = FALSE, npaths = 5000, x = train_df) ## ## Residual sd: 3.768 ## ## Error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set NaN NaN NaN NaN NaN NaN NA ## ## Forecasts: ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 135 24.83720 22.93389 26.72396 21.900597 27.70052 ## 136 24.82439 22.12656 27.48896 20.645047 28.85615 ## 137 24.81159 21.50122 28.07177 19.666381 29.73381 ## 138 24.79879 20.96989 28.56037 18.830186 30.46789 ## 139 24.78598 20.49852 28.98884 18.084426 31.11038 ## 140 24.77318 20.06963 29.37461 17.402417 31.68792 ## 141 24.76038 19.67285 29.72807 16.768303 32.21633 ## 142 24.74757 19.30144 30.05597 16.171745 32.70591 ## 143 24.73477 18.95069 30.36299 15.605550 33.16379 ## 144 24.72197 18.61720 30.65255 15.064469 33.59517 ## 145 24.70916 18.29839 30.92722 14.544525 34.00399 ## 146 24.69636 17.99224 31.18900 14.042617 34.39327 ## 147 24.68356 17.69718 31.43948 13.556263 34.76544 ## 148 24.67075 17.41190 31.67995 13.083435 35.12246 ## 149 24.65795 17.13535 31.91148 12.622447 35.46594 ## 150 24.64515 16.86663 32.13494 12.171871 35.79723 ## 151 24.63234 16.60498 32.35109 11.730485 36.11747 ## 152 24.61954 16.34977 32.56057 11.297220 36.42763 ## 153 24.60674 16.10044 32.76393 10.871139 36.72856 ## 154 24.59393 15.85651 32.96165 10.451401 37.02097 ## 155 24.58113 15.61754 33.15416 10.037246 37.30552 ## 156 24.56833 15.38317 33.34182 9.627980 37.58275 ## 157 24.55552 15.15305 33.52497 9.222955 37.85318 ## 158 24.54272 14.92689 33.70389 8.821563 38.11725 ## 159 24.52992 14.70443 33.87886 8.423225 38.37536 ## 160 24.51711 14.48542 34.05010 8.027378 38.62787 ## 161 24.50431 14.26964 34.21784 7.633467 38.87509 ## 162 24.49151 14.05689 34.38227 7.240938 39.11734 ## 163 24.47870 13.84700 34.54356 6.849226 39.35487 ## 164 24.46590 13.63978 34.70189 6.457743 39.58794 ## 165 24.45310 13.43509 34.85739 6.065865 39.81676 ## 166 24.44029 13.23279 35.01020 5.672920 40.04155 ## 167 24.42749 13.03274 35.16046 5.278158 40.26249 ## 168 24.41469 12.83482 35.30827 4.880733 40.47976  Stl model Below I show the specification on this model.\nstl_model \u0026lt;- stl(ts(train_df,frequency = 12,start = 1976), s.window = 12, t.window = 12, t.jump = 1) summary(stl_model) ## Call: ## stl(x = ts(train_df, frequency = 12, start = 1976), s.window = 12, ## t.window = 12, t.jump = 1) ## ## Time.series components: ## seasonal trend remainder ## Min. :-2.1696016 Min. :21.90544 Min. :-1.4192859 ## 1st Qu.:-0.6866532 1st Qu.:22.59791 1st Qu.:-0.2434104 ## Median :-0.0220175 Median :24.40128 Median : 0.0203580 ## Mean :-0.0179313 Mean :24.44334 Mean :-0.0091438 ## 3rd Qu.: 0.8119665 3rd Qu.:25.78991 3rd Qu.: 0.2328295 ## Max. : 1.5454074 Max. :27.53841 Max. : 1.3195528 ## IQR: ## STL.seasonal STL.trend STL.remainder data ## 1.4986 3.1920 0.4762 3.1375 ## % 47.8 101.7 15.2 100.0 ## ## Weights: all == 1 ## ## Other components: List of 5 ## $ win : Named num [1:3] 12 12 13 ## $ deg : Named int [1:3] 0 1 1 ## $ jump : Named num [1:3] 2 1 2 ## $ inner: int 2 ## $ outer: int 0  Holt-Winters (additive \u0026amp; multiplicative) The hw additive \u0026amp; multiplicative models was estimated with a Box-Cox transformation. Below I show the specification on this model. Moreover, they used a damped trend.\nhw_additive \u0026lt;- hw(ts(train_df,frequency = 12,start = 1976) , seasonal=\u0026quot;additive\u0026quot;, m= 12, lambda=\u0026quot;auto\u0026quot;, damped= TRUE, h=length(test_df)) cat(\u0026#39;additive model summary\\n\u0026#39;) ## additive model summary summary(hw_additive) ## ## Forecast method: Damped Holt-Winters\u0026#39; additive method ## ## Model Information: ## Damped Holt-Winters\u0026#39; additive method ## ## Call: ## hw(y = ts(train_df, frequency = 12, start = 1976), h = length(test_df), ## ## Call: ## seasonal = \u0026quot;additive\u0026quot;, damped = TRUE, lambda = \u0026quot;auto\u0026quot;, m = 12) ## ## Box-Cox transformation: lambda= 0.6711 ## ## Smoothing parameters: ## alpha = 0.8278 ## beta = 2e-04 ## gamma = 0.0075 ## phi = 0.9098 ## ## Initial states: ## l = 12.3481 ## b = -0.2367 ## s = -0.1088 -0.3909 0.2734 0.211 0.3974 0.4549 ## -0.0421 0.0671 -0.2706 0.331 -0.7103 -0.2121 ## ## sigma: 0.2168 ## ## AIC AICc BIC ## 264.4321 270.3799 316.5932 ## ## Error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.05520405 0.5791917 0.4503309 0.1990123 1.842102 0.4544282 ## ACF1 ## Training set 0.05843258 ## ## Forecasts: ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## Mar 1987 27.96634 27.13937 28.80142 26.70492 29.24675 ## Apr 1987 26.18557 25.13679 27.24834 24.58734 27.81654 ## May 1987 27.18369 25.92567 28.46115 25.26770 29.14517 ## Jun 1987 26.85080 25.42976 28.29702 24.68786 29.07265 ## Jul 1987 28.34337 26.74367 29.97335 25.90929 30.84826 ## Aug 1987 28.15025 26.41552 29.92088 25.51201 30.87247 ## Sep 1987 27.59537 25.74497 29.48753 24.78266 30.50575 ## Oct 1987 27.78772 25.81390 29.80877 24.78856 30.89739 ## Nov 1987 25.83006 23.79502 27.91928 22.74018 29.04668 ## Dec 1987 26.64753 24.48632 28.86801 23.36683 30.06690 ## Jan 1988 26.35591 24.10351 28.67350 22.93824 29.92610 ## Feb 1988 24.90024 22.59782 27.27494 21.40907 28.56052 ## Mar 1988 27.96621 25.47670 30.53087 24.19008 31.91817 ## Apr 1988 26.18546 23.66360 28.78988 22.36302 30.20110 ## May 1988 27.18359 24.54406 29.91027 23.18311 31.38801 ## Jun 1988 26.85071 24.14000 29.65461 22.74393 31.17555 ## Jul 1988 28.34329 25.50097 31.28265 24.03682 32.87683 ## Aug 1988 28.15017 25.23580 31.16738 23.73598 32.80501 ## Sep 1988 27.59530 24.62472 30.67505 23.09783 32.34822 ## Oct 1988 27.78765 24.73588 30.95393 23.16829 32.67497 ## Nov 1988 25.83001 22.78316 29.00001 21.22192 30.72630 ## Dec 1988 26.64748 23.49843 29.92406 21.88493 31.70848 ## Jan 1989 26.35586 23.15103 29.69441 21.51067 31.51400 ## Feb 1989 24.90020 21.69252 28.25004 20.05433 30.07880 ## Mar 1989 27.96617 24.56058 31.51408 22.81755 33.44785 ## Apr 1989 26.18542 22.79270 29.72941 21.06042 31.66449 ## May 1989 27.18356 23.68401 30.83809 21.89672 32.83316 ## Jun 1989 26.85068 23.30454 30.55814 21.49532 32.58363 ## Jul 1989 28.34326 24.66896 32.18148 22.79294 34.27725 ## Aug 1989 28.15015 24.42424 32.04594 22.52348 34.17446 ## Sep 1989 27.59528 23.83607 31.53116 21.92063 33.68347 ## Oct 1989 27.78763 23.96143 31.79569 22.01275 33.98820 ## Nov 1989 25.82999 22.04370 29.80850 20.12077 31.98921 ## Dec 1989 26.64746 22.76480 30.72599 20.79236 32.96108 hw_multiplicative \u0026lt;- hw(ts(train_df,frequency = 12,start = 1976), seasonal=\u0026quot;multiplicative\u0026quot;, m= 12, lambda=NULL, damped= TRUE,h=length(test_df)) cat(\u0026#39;multiplicative model summary\\n\u0026#39;) ## multiplicative model summary summary(hw_multiplicative) ## ## Forecast method: Damped Holt-Winters\u0026#39; multiplicative method ## ## Model Information: ## Damped Holt-Winters\u0026#39; multiplicative method ## ## Call: ## hw(y = ts(train_df, frequency = 12, start = 1976), h = length(test_df), ## ## Call: ## seasonal = \u0026quot;multiplicative\u0026quot;, damped = TRUE, lambda = NULL, ## ## Call: ## m = 12) ## ## Smoothing parameters: ## alpha = 0.7541 ## beta = 0.0087 ## gamma = 1e-04 ## phi = 0.9675 ## ## Initial states: ## l = 25.5802 ## b = -0.2795 ## s = 0.9832 0.9532 1.0286 1.0205 1.0446 1.0573 ## 0.9978 1.0129 0.9686 1.0385 0.9153 0.9795 ## ## sigma: 0.027 ## ## AIC AICc BIC ## 560.6326 566.5804 612.7937 ## ## Error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 0.07476038 0.6071546 0.4533434 0.2795672 1.84821 0.4574681 ## ACF1 ## Training set 0.1146113 ## ## Forecasts: ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## Mar 1987 28.22983 27.25318 29.20648 26.73617 29.72349 ## Apr 1987 26.33558 25.18961 27.48154 24.58297 28.08818 ## May 1987 27.54943 26.14322 28.95564 25.39882 29.70004 ## Jun 1987 27.14538 25.57837 28.71240 24.74884 29.54193 ## Jul 1987 28.77026 26.93389 30.60663 25.96177 31.57874 ## Aug 1987 28.43224 26.45620 30.40829 25.41014 31.45435 ## Sep 1987 27.78380 25.70455 29.86305 24.60386 30.96374 ## Oct 1987 28.01173 25.77349 30.24998 24.58863 31.43484 ## Nov 1987 25.96344 23.76309 28.16378 22.59830 29.32857 ## Dec 1987 26.78599 24.39123 29.18074 23.12352 30.44845 ## Jan 1988 26.69138 24.18515 29.19762 22.85843 30.52434 ## Feb 1988 24.94746 22.49640 27.39852 21.19889 28.69603 ## Mar 1988 28.31180 25.41048 31.21311 23.87462 32.74898 ## Apr 1988 26.40953 23.59446 29.22461 22.10424 30.71483 ## May 1988 27.62426 24.56876 30.67976 22.95128 32.29724 ## Jun 1988 27.21669 24.09945 30.33394 22.44928 31.98411 ## Jul 1988 28.84336 25.42898 32.25774 23.62152 34.06520 ## Aug 1988 28.50212 25.02080 31.98344 23.17790 33.82634 ## Sep 1988 27.84984 24.34527 31.35442 22.49006 33.20963 ## Oct 1988 28.07614 24.44116 31.71111 22.51692 33.63535 ## Nov 1988 26.02117 22.55937 29.48297 20.72681 31.31554 ## Dec 1988 26.84360 23.17811 30.50909 21.23772 32.44949 ## Jan 1989 26.74691 23.00212 30.49171 21.01975 32.47408 ## Feb 1989 24.99766 21.41259 28.58273 19.51477 30.48055 ## Mar 1989 28.36691 24.20330 32.53052 21.99921 34.73460 ## Apr 1989 26.45926 22.48794 30.43057 20.38566 32.53285 ## May 1989 27.67457 23.43031 31.91882 21.18354 34.16559 ## Jun 1989 27.26464 22.99518 31.53409 20.73507 33.79420 ## Jul 1989 28.89250 24.27595 33.50906 21.83209 35.95292 ## Aug 1989 28.54910 23.89739 33.20080 21.43492 35.66327 ## Sep 1989 27.89424 23.26229 32.52620 20.81028 34.97821 ## Oct 1989 28.11943 23.36341 32.87545 20.84573 35.39314 ## Nov 1989 26.05999 21.57284 30.54714 19.19749 32.92249 ## Dec 1989 26.88233 22.17250 31.59217 19.67926 34.08541   Predictions of test data Then, I saved the predictions of the test data for each model.\npred_arima \u0026lt;- forecast(arima_model, h = length(test_df)) pred_drift \u0026lt;- drift_model pred_naive \u0026lt;- naive_model pred_snaive \u0026lt;- snaive_model pred_stl \u0026lt;- forecast(stl_model,h= length(test_df)) pred_hw_add \u0026lt;- hw_additive pred_hw_mult \u0026lt;- hw_multiplicative  Compare results of each models Here, I made a graph in order to compare the models performance.\narima_compare = create_df_compare(test_df, c(pred_arima$mean)) drift_compare = create_df_compare(test_df, c(pred_drift$mean)) naive_compare = create_df_compare(test_df, c(pred_naive$mean)) snaive_compare = create_df_compare(test_df, c(pred_snaive$mean)) stl_compare = create_df_compare(test_df, c(pred_stl$mean)) hw_add_compare = create_df_compare(test_df, c(pred_hw_add$mean)) hw_mult_compare = create_df_compare(test_df, c(pred_hw_mult$mean)) plot_df(arima_compare, \u0026quot;Results Real vs Arima\u0026quot; ) plot_df(drift_compare, \u0026quot;Results Real vs drift\u0026quot; ) plot_df(naive_compare, \u0026quot;Results Real vs Naive\u0026quot; ) plot_df(snaive_compare, \u0026quot;Results Real vs Snaive\u0026quot; ) plot_df(stl_compare, \u0026quot;Results Real vs Stl\u0026quot; ) plot_df(hw_add_compare, \u0026quot;Results Real vs Hw. Add.\u0026quot; ) plot_df(hw_mult_compare, \u0026quot;Results Real vs Hw. Mult.\u0026quot; )  RMSE for each model Now, I compute the rmse using the test data and the predictions of each model.\nrmse_arima \u0026lt;- rmse( c(pred_arima$mean),test_df ) rmse_drift \u0026lt;- rmse( c(pred_drift$mean), test_df ) rmse_naive \u0026lt;- rmse( c(pred_naive$mean), test_df ) rmse_snaive \u0026lt;- rmse( c(pred_snaive$mean), test_df ) rmse_stl \u0026lt;- rmse( c(pred_stl$mean), test_df ) rmse_hw_add \u0026lt;- rmse( c(pred_hw_add$mean), test_df ) rmse_hw_mult \u0026lt;- rmse( c(pred_hw_mult$mean), test_df )  Results I saved the results in a data frame sorted by rmse in ascending order where the first row is the best model with the lowest rmse. The best model was hw multiplicative with a rmse of 0.78 and the worst was drift model with a rmse of 3.49.\ndf_rmse \u0026lt;- data.frame(method= c(\u0026#39;arima\u0026#39;, \u0026#39;drift\u0026#39;, \u0026#39;naive\u0026#39;, \u0026#39;snaive\u0026#39;, \u0026#39;stl\u0026#39;, \u0026#39;hw_additive\u0026#39;, \u0026#39;hw_multiplicative\u0026#39;), rmse_values = c(rmse_arima, rmse_drift, rmse_naive, rmse_snaive, rmse_stl, rmse_hw_add, rmse_hw_mult) ) %\u0026gt;% arrange((rmse_values)) df_rmse ## method rmse_values ## 1 hw_multiplicative 0.7818894 ## 2 stl 0.8481321 ## 3 arima 0.8754021 ## 4 hw_additive 0.8968844 ## 5 naive 3.2261873 ## 6 snaive 3.2261873 ## 7 drift 3.4999880  ","date":1644459194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644459194,"objectID":"9a6cf90fd61ff9ec02d3f8a91702441b","permalink":"https://alexamaguaya.netlify.app/blog/time_series_methods/","publishdate":"2022-02-09T21:13:14-05:00","relpermalink":"/blog/time_series_methods/","section":"blog","summary":"This blog shows the application of some time series methods with an example.","tags":["R","Time Series"],"title":"Time Series Models","type":"blog"},{"authors":["Jos√© Gabriel Castillo","Cinthy Veintimilla","N√©mesis G√≥mez","Alex Amaguaya"],"categories":null,"content":"","date":1637366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637366400,"objectID":"2bc7ef24f7a01da494312a88f8f32a5c","permalink":"https://alexamaguaya.netlify.app/publication/report_cip/","publishdate":"2021-10-26T00:00:00Z","relpermalink":"/publication/report_cip/","section":"publication","summary":"The Galapagos Islands are internationally recognized as a world-class tourist destination; however, little research has been done on the economic dynamism that tourism activities generate at the local and national levels. This research is based on secondary information from official institutions, and primary information that included interviews with cruise ship representatives and experts, taking 2019 as the year of analysis, since 2020 presents an atypical dynamic due to the Covid-19 pandemic. To identify the economic contribution of cruise ships, we began with an analysis of the economic contribution of the tourism industry at the national level and, in particular, in the Galapagos archipelago, followed by a projection of the economic contribution of cruise ship tourism to the Galapagos Islands and, finally, an estimate of the economic dynamics generated by cruise ship tourism on the mainland.","tags":["Applied Econometrics","Applied Economics"],"title":"Contribuci√≥n Econ√≥mica del turismo de Crucero Navegable en el Archipi√©lago de Gal√°pagos y el pa√≠s","type":"publication"},{"authors":null,"categories":"R","content":"  body { font-size: 14pt; } h1 { /* Header 2 */ font-size: 32px; color: #cf0a0d; font-weight: bold; } h2 { /* Header 1 */ font-size: 24px; color: #d3ad12; font-weight: bold; }  Formation of Treated and Control groups for Causal Inference MatchIt is a R package for processing data using nonparametric matching methods to improve the estimation of parametric statistical models.\nSee more information about package here.\nIn this example I use a lalonde data set, where each row has the data of a person with some features and a treatment variable (job training program). The goal of this example is to evaluate the effectiveness of the treatment on an individual‚Äôs income a few years after completing the program.\nFirst, I show statistical metrics about the data set, where:\n treat is a binary variable and if it‚Äôs equal to 1 a person took the treatment and 0 is the opposite case. age is the age of a person. educ is the years of schooling of a person.\n race is a categorical variable and represent the race of a person, e.g.¬†black, hispan, white. married is a binary variable and if it‚Äôs equall to 1 a person is married and 0 is the opposite case. nodegree is a binary variable and if it‚Äôs equall to 1 a person had not a degree and 0 is the opposite case. re74 is the real earnings in 1974. re75 is the real earnings in 1975.\n re78 is the real earnings in 1978.  library(\u0026quot;MatchIt\u0026quot;) data(\u0026quot;lalonde\u0026quot;) summary(lalonde) ## treat age educ race married ## Min. :0.0000 Min. :16.00 Min. : 0.00 black :243 Min. :0.0000 ## 1st Qu.:0.0000 1st Qu.:20.00 1st Qu.: 9.00 hispan: 72 1st Qu.:0.0000 ## Median :0.0000 Median :25.00 Median :11.00 white :299 Median :0.0000 ## Mean :0.3013 Mean :27.36 Mean :10.27 Mean :0.4153 ## 3rd Qu.:1.0000 3rd Qu.:32.00 3rd Qu.:12.00 3rd Qu.:1.0000 ## Max. :1.0000 Max. :55.00 Max. :18.00 Max. :1.0000 ## nodegree re74 re75 re78 ## Min. :0.0000 Min. : 0 Min. : 0.0 Min. : 0.0 ## 1st Qu.:0.0000 1st Qu.: 0 1st Qu.: 0.0 1st Qu.: 238.3 ## Median :1.0000 Median : 1042 Median : 601.5 Median : 4759.0 ## Mean :0.6303 Mean : 4558 Mean : 2184.9 Mean : 6792.8 ## 3rd Qu.:1.0000 3rd Qu.: 7888 3rd Qu.: 3249.0 3rd Qu.:10893.6 ## Max. :1.0000 Max. :35040 Max. :25142.2 Max. :60307.9 Now, I process the data using fastDummies library to create binary variables of the categorical feature (race) and rename these variables. Moreover, in this example I only use the re78 variable.\nlibrary(dplyr) ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union library(fastDummies) lalonde \u0026lt;- dummy_cols(lalonde, select_columns = c(\u0026#39;race\u0026#39;),remove_selected_columns = TRUE) %\u0026gt;% rename( black = race_black, hispan = race_hispan, white= race_white ) Exploratory Data Analysis The average income of the Control Group is more bigger than the Treatment Group in 1974 (5619 vs 2095)\ntable_comp_74 \u0026lt;-lalonde %\u0026gt;% group_by(treat) %\u0026gt;% summarise_at( vars(re74), list(mean = mean, median = median )) head(table_comp_74) ## # A tibble: 2 √ó 3 ## treat mean median ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 5619. 2547. ## 2 1 2096. 0 The difference in mean income of the Control and Treatment groups appear to be reduced in 1978. (6984 vs 6349)\ntable_comp_78 \u0026lt;-lalonde %\u0026gt;% group_by(treat) %\u0026gt;% summarise_at( vars(re78), list(mean = mean, median = median )) head(table_comp_78) ## # A tibble: 2 √ó 3 ## treat mean median ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 6984. 4976. ## 2 1 6349. 4232. The control group has higher earning that the treatment group - does this mean the treatment had a negative impact?\nlibrary(scales) par(mfrow=c(3,1), mar=c(3,4,3,1)) lalonde %\u0026gt;% filter( treat ==1 ) %\u0026gt;% with(hist(re74 ,col=\u0026#39;skyblue\u0026#39;,border=FALSE,breaks= 20, main=\u0026#39;Earning distribution of treatment and control 74\u0026#39;)) lalonde %\u0026gt;% filter( treat ==0 ) %\u0026gt;% with(hist(re74 ,col=scales::alpha(\u0026#39;red\u0026#39;,.5),breaks= 20, border=FALSE, add=TRUE, main=\u0026#39;Earning distribution of treatment and control 74\u0026#39;)) legend(x=30000,y=100,c(\u0026quot;treatment\u0026quot;,\u0026quot;control\u0026quot;),cex=.8,col=c(\u0026quot;skyblue\u0026quot;,scales::alpha(\u0026#39;red\u0026#39;,.5)), fill=c(\u0026quot;skyblue\u0026quot;,scales::alpha(\u0026#39;red\u0026#39;,.5))) lalonde %\u0026gt;% filter( treat ==1 ) %\u0026gt;% with(hist(educ ,col=\u0026#39;skyblue\u0026#39;,border=FALSE,breaks= 20, main=\u0026#39;Educ. distribution of treatment and control\u0026#39;)) lalonde %\u0026gt;% filter( treat ==0 ) %\u0026gt;% with(hist(educ ,col=scales::alpha(\u0026#39;red\u0026#39;,.5),breaks= 20, border=FALSE, add=TRUE, main=\u0026#39;Edu. of treatment and control\u0026#39;)) legend(x=14,y=40,c(\u0026quot;treatment\u0026quot;,\u0026quot;control\u0026quot;),cex=.8,col=c(\u0026quot;skyblue\u0026quot;,scales::alpha(\u0026#39;red\u0026#39;,.5)), fill=c(\u0026quot;skyblue\u0026quot;,scales::alpha(\u0026#39;red\u0026#39;,.5))) lalonde %\u0026gt;% filter( treat ==1 ) %\u0026gt;% with(hist(age ,col=\u0026#39;skyblue\u0026#39;,border=FALSE,breaks= 20, main=\u0026#39;Age distribution of treatment and control\u0026#39;)) lalonde %\u0026gt;% filter( treat ==0 ) %\u0026gt;% with(hist(age ,col=scales::alpha(\u0026#39;red\u0026#39;,.5),breaks= 20, border=FALSE, add=TRUE, main=\u0026#39;Age of treatment and control\u0026#39;)) legend(x=45,y=24,c(\u0026quot;treatment\u0026quot;,\u0026quot;control\u0026quot;),cex=.8,col=c(\u0026quot;skyblue\u0026quot;,scales::alpha(\u0026#39;red\u0026#39;,.5)), fill=c(\u0026quot;skyblue\u0026quot;,scales::alpha(\u0026#39;red\u0026#39;,.5)))  Matching Process After processing data, I run an logit model to analysis some previous results about Propensity score model. It used the treatment variable like depend variable and the others variables are independent variables. The equation looks something like this:\n\\[P\\left(treatment_{i}=1|X\\right) = age_{i}+edu_{i}+married_{i}+nodegree_{i}+re78_{i}+black_{i}+hispan_{i}\\]\nI estimate that equation and analyze the beta coefficients and their statistical significant. Also, the white dummy variable is omitted when I estimate the equation.\nThe results show:\n educ, nodegree, black, hispan have a positive relationship with the treatment variable and are statistically significant. married has a negative relationship with the treatment variable and is statistically significant.  library(glm2) propensity.score.model \u0026lt;- glm(treat ~ age+educ+married+nodegree+black+hispan,family=binomial() , data= lalonde) pscore \u0026lt;- propensity.score.model$fitted.values summary(propensity.score.model) ## ## Call: ## glm(formula = treat ~ age + educ + married + nodegree + black + ## hispan, family = binomial(), data = lalonde) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7709 -0.4606 -0.2963 0.7766 2.6384 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -4.67874 1.02120 -4.582 4.61e-06 *** ## age 0.01030 0.01329 0.775 0.43857 ## educ 0.15161 0.06568 2.308 0.02098 * ## married -0.92969 0.27128 -3.427 0.00061 *** ## nodegree 0.78719 0.33507 2.349 0.01881 * ## black 3.12657 0.28514 10.965 \u0026lt; 2e-16 *** ## hispan 0.99947 0.42191 2.369 0.01784 * ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 751.49 on 613 degrees of freedom ## Residual deviance: 494.70 on 607 degrees of freedom ## AIC: 508.7 ## ## Number of Fisher Scoring iterations: 5 After analyzing the results, I apply the matching method to form the new control and treatment groups. The following shows the parameters used by the function:\nmatchit(formula, data, method = \u0026quot;nearest\u0026quot;, distance = \u0026quot;logit\u0026quot;, distance.options = list(), discard = \u0026quot;none\u0026quot;, reestimate = FALSE, ...) Where: These are the main arguments:\n formula: this argument takes the usual syntax of R formula. data: this argument specifies the used data. method: this argument specifies a matching method.  ‚Äúexact‚Äù (exact matching) ‚Äúfull‚Äù (full matching) ‚Äúgenetic‚Äù (genetic matching) ‚Äúnearest‚Äù (nearest neighbor matching), it‚Äôs the default option. ‚Äúoptimal‚Äù (optimal matching) ‚Äúsubclass‚Äù (subclassification)  distance: this argument specifies the method used to estimate the distance measure (logistic regression ‚Äúlogit‚Äù is the default option). discard: this argument specifies whether to discard units that fall outside some measure of support of the distance score before matching, and not allow them to be used at all in the matching procedure. Note that discarding units may change the quantity of interest being estimated. The options are:  ‚Äúnone‚Äù is the default, which discards no units before matching ‚Äúboth‚Äù which discards all units (treated and control) that are outside the support of the distance measure ‚Äúcontrol‚Äù which discards only control units outside the support of the distance measure of the treated units ‚Äútreat‚Äù which discards only treated units outside the support of the distance measure of the control units.  reestimate: this argument specifies whether the model for distance measure should be re-estimated after units are discarded. The input must be a logical value. The default is FALSE. ratio: it‚Äôs about how many control units should be matched to each treated unit in k:1 matching.  Here, I apply the matchit process with a ratio of 2.\nmatch_method \u0026lt;- matchit(treat ~ age+educ+married+nodegree+black+hispan , data=lalonde, method=\u0026#39;nearest\u0026#39;,ratio= 2 ) match_method ## A matchit object ## - method: Variable ratio 2:1 nearest neighbor matching without replacement ## - distance: Propensity score ## - estimated with logistic regression ## - number of obs.: 614 (original), 555 (matched) ## - target estimand: ATT ## - covariates: age, educ, married, nodegree, black, hispan Then, I show the propensity score distribution before and after matching. You can see that there are many observations with a probability or propensity score less than 0.15.\nplot(match_method ,type=\u0026#39;jitter\u0026#39;) ## [1] \u0026quot;To identify the units, use first mouse button; to stop, use second.\u0026quot; ## integer(0) A histogram plot is shown below and you can see that the propensity score distribution for the treated and control groups doesn‚Äôt change significantly (before \u0026amp; after Matching)\nplot(match_method, type=\u0026#39;hist\u0026#39;, col.axis=4 ) Mean difference of each variable for before and after match process\nlibrary(cobalt) love.plot(bal.tab(match_method,binary=\u0026#39;std\u0026#39;, m.threshold=0.3), stat=\u0026#39;mean.diffs\u0026#39;, abs=F, shapes=c(\u0026#39;triangle filled\u0026#39;, \u0026#39;circle filled\u0026#39;), colors = c(\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;), sample.names = c(\u0026quot;Before Match\u0026quot;, \u0026quot;After Match\u0026quot;) ) Now, I save the new treatment and control groups and can used for estimate model.\nnew_treat \u0026lt;- match.data(match_method, group=\u0026#39;treat\u0026#39;) new_control \u0026lt;- match.data(match_method, group=\u0026#39;control\u0026#39;)   ","date":1636510394,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636510394,"objectID":"c98534f4e765e6b461028371b061405e","permalink":"https://alexamaguaya.netlify.app/blog/causal-inference/","publishdate":"2021-11-09T21:13:14-05:00","relpermalink":"/blog/causal-inference/","section":"blog","summary":"This blog shows an example about using **MachtIt package** with **Lalonde data set**.","tags":["R","Causal Inference","Microeconomics"],"title":"MatchIt Example: Nonparametric Preprocessing for Parametric Causal Inference","type":"blog"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://alexamaguaya.netlify.app/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://alexamaguaya.netlify.app/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]